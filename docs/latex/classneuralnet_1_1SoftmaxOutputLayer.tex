\hypertarget{classneuralnet_1_1SoftmaxOutputLayer}{}\section{neuralnet\+:\+:Softmax\+Output\+Layer Class Reference}
\label{classneuralnet_1_1SoftmaxOutputLayer}\index{neuralnet\+::\+Softmax\+Output\+Layer@{neuralnet\+::\+Softmax\+Output\+Layer}}


Output layer with Softmax activation function and cross entropy loss function.  




{\ttfamily \#include $<$softmax\+\_\+output\+\_\+layer.\+hpp$>$}



Inheritance diagram for neuralnet\+:\+:Softmax\+Output\+Layer\+:
% FIG 0


Collaboration diagram for neuralnet\+:\+:Softmax\+Output\+Layer\+:
% FIG 1
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
double \hyperlink{classneuralnet_1_1SoftmaxOutputLayer_a6cc46c109780dca4700f622b2a378aa8}{Get\+Loss} (const std\+::vector$<$ double $>$ \&target\+\_\+output)
\begin{DoxyCompactList}\small\item\em Computes cross entropy loss. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Protected Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\Hypertarget{classneuralnet_1_1SoftmaxOutputLayer_a80635dc564f1edc46090c820607243d1}\label{classneuralnet_1_1SoftmaxOutputLayer_a80635dc564f1edc46090c820607243d1}} 
void \hyperlink{classneuralnet_1_1SoftmaxOutputLayer_a80635dc564f1edc46090c820607243d1}{Initialize\+Weights} ()
\begin{DoxyCompactList}\small\item\em Initializes all weights to 0. \end{DoxyCompactList}\item 
void \hyperlink{classneuralnet_1_1SoftmaxOutputLayer_a6a18e1e7fc94cb7f3d66dfad2dcb1f85}{Forward\+Prop\+Cpu} (const std\+::vector$<$ double $>$ \&input)
\begin{DoxyCompactList}\small\item\em Given input computes neurons\textquotesingle{} activations and applies softmax function using cpu. \end{DoxyCompactList}\item 
void \hyperlink{classneuralnet_1_1SoftmaxOutputLayer_acb4f6f8739a3dedde111f11b548899f7}{Forward\+Prop\+Gpu} (const std\+::vector$<$ double $>$ \&input)
\begin{DoxyCompactList}\small\item\em Given input computes neurons\textquotesingle{} activations and applies softmax function using gpu. \end{DoxyCompactList}\item 
void \hyperlink{classneuralnet_1_1SoftmaxOutputLayer_a7360169d36832228b4938f9ff469bb59}{Back\+Prop\+Cpu} (const std\+::vector$<$ double $>$ \&target\+\_\+output, const std\+::vector$<$ double $>$ \&prev\+\_\+layer\+\_\+output, double momentum)
\begin{DoxyCompactList}\small\item\em Computes velocity of weights and weighted error of this layer using cpu. \end{DoxyCompactList}\item 
void \hyperlink{classneuralnet_1_1SoftmaxOutputLayer_af3308eebc5a7acc982f4ac0b55d34fc3}{Back\+Prop\+Gpu} (const std\+::vector$<$ double $>$ \&target\+\_\+output, const std\+::vector$<$ double $>$ \&prev\+\_\+layer\+\_\+output, double momentum)
\begin{DoxyCompactList}\small\item\em Computes velocity of weights and weighted error of this layer using gpu. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Additional Inherited Members}


\subsection{Detailed Description}
Output layer with Softmax activation function and cross entropy loss function. 

Softmax layer returns probability distribution.

Softmax function is defined as follows\+: $softmax(a_{j}) = \frac{e^{a_{j}}}{\sum_{i}e^{a_{i}}} $ , whereas cross entropy is defined as\+: $L = -y * \log{p} $ , where\+: y\+: vector of desired outputs p\+: vector of actual outputs 

\subsection{Member Function Documentation}
\mbox{\Hypertarget{classneuralnet_1_1SoftmaxOutputLayer_a7360169d36832228b4938f9ff469bb59}\label{classneuralnet_1_1SoftmaxOutputLayer_a7360169d36832228b4938f9ff469bb59}} 
\index{neuralnet\+::\+Softmax\+Output\+Layer@{neuralnet\+::\+Softmax\+Output\+Layer}!Back\+Prop\+Cpu@{Back\+Prop\+Cpu}}
\index{Back\+Prop\+Cpu@{Back\+Prop\+Cpu}!neuralnet\+::\+Softmax\+Output\+Layer@{neuralnet\+::\+Softmax\+Output\+Layer}}
\subsubsection{\texorpdfstring{Back\+Prop\+Cpu()}{BackPropCpu()}}
{\footnotesize\ttfamily void neuralnet\+::\+Softmax\+Output\+Layer\+::\+Back\+Prop\+Cpu (\begin{DoxyParamCaption}\item[{const std\+::vector$<$ double $>$ \&}]{target\+\_\+output,  }\item[{const std\+::vector$<$ double $>$ \&}]{prev\+\_\+layer\+\_\+output,  }\item[{double}]{momentum }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}, {\ttfamily [virtual]}}



Computes velocity of weights and weighted error of this layer using cpu. 

Backpropagates through this layer using cross entropy as a loss function. Cross entropy is defined as\+: $L = -y * \log{p} $ , where\+: y\+: vector of desired outputs p\+: vector of actual outputs

This function writes to error\+\_\+, velocity\+\_\+ and weighted\+\_\+error\+\_\+. This functions accepts mini-\/batch laid in matrix columns stored in a vector using row major ordering and computes a mini-\/batch of output.


\begin{DoxyParams}{Parameters}
{\em target\+\_\+output} & Correct output to most recent minibatch of input. \\
\hline
{\em prev\+\_\+layer\+\_\+output} & Return value of forward step in previous layer. \\
\hline
{\em momentum} & Momentum coefficient for velocity calculation. \\
\hline
\end{DoxyParams}

\begin{DoxyExceptions}{Exceptions}
{\em std\+::invalid\+\_\+argument} & If momentum value lies outside of (0, 1) set. \\
\hline
\end{DoxyExceptions}


Implements \hyperlink{classneuralnet_1_1Layer_acb789462daab9227ff4ce6f7332bd38c}{neuralnet\+::\+Layer}.

\mbox{\Hypertarget{classneuralnet_1_1SoftmaxOutputLayer_af3308eebc5a7acc982f4ac0b55d34fc3}\label{classneuralnet_1_1SoftmaxOutputLayer_af3308eebc5a7acc982f4ac0b55d34fc3}} 
\index{neuralnet\+::\+Softmax\+Output\+Layer@{neuralnet\+::\+Softmax\+Output\+Layer}!Back\+Prop\+Gpu@{Back\+Prop\+Gpu}}
\index{Back\+Prop\+Gpu@{Back\+Prop\+Gpu}!neuralnet\+::\+Softmax\+Output\+Layer@{neuralnet\+::\+Softmax\+Output\+Layer}}
\subsubsection{\texorpdfstring{Back\+Prop\+Gpu()}{BackPropGpu()}}
{\footnotesize\ttfamily void neuralnet\+::\+Softmax\+Output\+Layer\+::\+Back\+Prop\+Gpu (\begin{DoxyParamCaption}\item[{const std\+::vector$<$ double $>$ \&}]{target\+\_\+output,  }\item[{const std\+::vector$<$ double $>$ \&}]{prev\+\_\+layer\+\_\+output,  }\item[{double}]{momentum }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}, {\ttfamily [virtual]}}



Computes velocity of weights and weighted error of this layer using gpu. 

Backpropagates through this layer using cross entropy as a loss function. Cross entropy is defined as\+: $L = -y * \log{p} $ , where\+: y\+: vector of desired outputs p\+: vector of actual outputs

This function writes to error\+\_\+, velocity\+\_\+ and weighted\+\_\+error\+\_\+. This functions accepts mini-\/batch laid in matrix columns stored in a vector using row major ordering and computes a mini-\/batch of output.


\begin{DoxyParams}{Parameters}
{\em target\+\_\+output} & Correct output to most recent minibatch of input. \\
\hline
{\em prev\+\_\+layer\+\_\+output} & Return value of forward step in previous layer. \\
\hline
{\em momentum} & Momentum coefficient for velocity calculation. \\
\hline
\end{DoxyParams}

\begin{DoxyExceptions}{Exceptions}
{\em std\+::invalid\+\_\+argument} & If momentum value lies outside of (0, 1) set. \\
\hline
\end{DoxyExceptions}


Implements \hyperlink{classneuralnet_1_1Layer_aa81e9cfb0eaf5e17b38e011c4f56f042}{neuralnet\+::\+Layer}.

\mbox{\Hypertarget{classneuralnet_1_1SoftmaxOutputLayer_a6a18e1e7fc94cb7f3d66dfad2dcb1f85}\label{classneuralnet_1_1SoftmaxOutputLayer_a6a18e1e7fc94cb7f3d66dfad2dcb1f85}} 
\index{neuralnet\+::\+Softmax\+Output\+Layer@{neuralnet\+::\+Softmax\+Output\+Layer}!Forward\+Prop\+Cpu@{Forward\+Prop\+Cpu}}
\index{Forward\+Prop\+Cpu@{Forward\+Prop\+Cpu}!neuralnet\+::\+Softmax\+Output\+Layer@{neuralnet\+::\+Softmax\+Output\+Layer}}
\subsubsection{\texorpdfstring{Forward\+Prop\+Cpu()}{ForwardPropCpu()}}
{\footnotesize\ttfamily void neuralnet\+::\+Softmax\+Output\+Layer\+::\+Forward\+Prop\+Cpu (\begin{DoxyParamCaption}\item[{const std\+::vector$<$ double $>$ \&}]{input }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}, {\ttfamily [virtual]}}



Given input computes neurons\textquotesingle{} activations and applies softmax function using cpu. 

Softmax function is defined as follows\+: $softmax(a_{j}) = \frac{e^{a_{j}}}{\sum_{i}e^{a_{i}}} $.

Writes to activation\+\_\+ and output\+\_\+. This functions accepts mini-\/batch laid in matrix columns stored in a vector using row major ordering and computes a mini-\/batch of output.


\begin{DoxyParams}{Parameters}
{\em input} & \hyperlink{classneuralnet_1_1Layer}{Layer}\textquotesingle{}s input. \\
\hline
\end{DoxyParams}


Implements \hyperlink{classneuralnet_1_1Layer_a3aa08517de6a73640cd0e511c134b231}{neuralnet\+::\+Layer}.

\mbox{\Hypertarget{classneuralnet_1_1SoftmaxOutputLayer_acb4f6f8739a3dedde111f11b548899f7}\label{classneuralnet_1_1SoftmaxOutputLayer_acb4f6f8739a3dedde111f11b548899f7}} 
\index{neuralnet\+::\+Softmax\+Output\+Layer@{neuralnet\+::\+Softmax\+Output\+Layer}!Forward\+Prop\+Gpu@{Forward\+Prop\+Gpu}}
\index{Forward\+Prop\+Gpu@{Forward\+Prop\+Gpu}!neuralnet\+::\+Softmax\+Output\+Layer@{neuralnet\+::\+Softmax\+Output\+Layer}}
\subsubsection{\texorpdfstring{Forward\+Prop\+Gpu()}{ForwardPropGpu()}}
{\footnotesize\ttfamily void neuralnet\+::\+Softmax\+Output\+Layer\+::\+Forward\+Prop\+Gpu (\begin{DoxyParamCaption}\item[{const std\+::vector$<$ double $>$ \&}]{input }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}, {\ttfamily [virtual]}}



Given input computes neurons\textquotesingle{} activations and applies softmax function using gpu. 

Softmax function is defined as follows\+: $softmax(a_{j}) = \frac{e^{a_{j}}}{\sum_{i}e^{a_{i}}} $.

Writes to activation\+\_\+ and output\+\_\+. This functions accepts mini-\/batch laid in matrix columns stored in a vector using row major ordering and computes a mini-\/batch of output.


\begin{DoxyParams}{Parameters}
{\em input} & \hyperlink{classneuralnet_1_1Layer}{Layer}\textquotesingle{}s input. \\
\hline
\end{DoxyParams}


Implements \hyperlink{classneuralnet_1_1Layer_abd0fdf1146eb28485349337e68ad7982}{neuralnet\+::\+Layer}.

\mbox{\Hypertarget{classneuralnet_1_1SoftmaxOutputLayer_a6cc46c109780dca4700f622b2a378aa8}\label{classneuralnet_1_1SoftmaxOutputLayer_a6cc46c109780dca4700f622b2a378aa8}} 
\index{neuralnet\+::\+Softmax\+Output\+Layer@{neuralnet\+::\+Softmax\+Output\+Layer}!Get\+Loss@{Get\+Loss}}
\index{Get\+Loss@{Get\+Loss}!neuralnet\+::\+Softmax\+Output\+Layer@{neuralnet\+::\+Softmax\+Output\+Layer}}
\subsubsection{\texorpdfstring{Get\+Loss()}{GetLoss()}}
{\footnotesize\ttfamily double neuralnet\+::\+Softmax\+Output\+Layer\+::\+Get\+Loss (\begin{DoxyParamCaption}\item[{const std\+::vector$<$ double $>$ \&}]{target\+\_\+output }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [virtual]}}



Computes cross entropy loss. 

Cross entropy is defined as\+: $L = -y * \log{p} $ , where\+: y\+: vector of desired outputs p\+: vector of actual outputs

This functions accepts mini-\/batch laid in matrix columns stored in a vector using row major ordering.


\begin{DoxyParams}{Parameters}
{\em target\+\_\+output} & Correct output to most recent minibatch of input. \\
\hline
\end{DoxyParams}


Reimplemented from \hyperlink{classneuralnet_1_1Layer_a2da35925d900e217848443a07791b575}{neuralnet\+::\+Layer}.



The documentation for this class was generated from the following files\+:\begin{DoxyCompactItemize}
\item 
include/layers/softmax\+\_\+output\+\_\+layer.\+hpp\item 
src/layers/softmax\+\_\+output\+\_\+layer.\+cpp\end{DoxyCompactItemize}
