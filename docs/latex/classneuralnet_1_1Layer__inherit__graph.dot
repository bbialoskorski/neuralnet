digraph "neuralnet::Layer"
{
  edge [fontname="Helvetica",fontsize="10",labelfontname="Helvetica",labelfontsize="10"];
  node [fontname="Helvetica",fontsize="10",shape=record];
  Node0 [label="neuralnet::Layer",height=0.2,width=0.4,color="black", fillcolor="grey75", style="filled", fontcolor="black"];
  Node0 -> Node1 [dir="back",color="midnightblue",fontsize="10",style="solid"];
  Node1 [label="neuralnet::ReLuLayer",height=0.2,width=0.4,color="black", fillcolor="white", style="filled",URL="$classneuralnet_1_1ReLuLayer.html",tooltip="Hidden layer with rectifier activation function. "];
  Node0 -> Node2 [dir="back",color="midnightblue",fontsize="10",style="solid"];
  Node2 [label="neuralnet::SigmoidOutput\lLayer",height=0.2,width=0.4,color="black", fillcolor="white", style="filled",URL="$classneuralnet_1_1SigmoidOutputLayer.html",tooltip="Output layer with Sigmoid activation function and mean squared error cost function. "];
  Node0 -> Node3 [dir="back",color="midnightblue",fontsize="10",style="solid"];
  Node3 [label="neuralnet::SoftmaxOutput\lLayer",height=0.2,width=0.4,color="black", fillcolor="white", style="filled",URL="$classneuralnet_1_1SoftmaxOutputLayer.html",tooltip="Output layer with Softmax activation function and cross entropy loss function. "];
}
